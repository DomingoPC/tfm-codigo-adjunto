{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos usando GPU: RAPIDs\n",
    "\n",
    "Mediante WSL podemos trabajar con la versión de Linux de RAPIDs. Con ella, hemos entrenado varios modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cuml\n",
    "from cupy import asnumpy\n",
    "from joblib import dump, load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# https://docs.rapids.ai/api/cuml/stable/api/\n",
    "from cuml.ensemble import RandomForestRegressor as cuRF\n",
    "from cuml.linear_model import LinearRegression as cuLR\n",
    "from cuml.linear_model import ElasticNet\n",
    "from cuml.svm import SVR\n",
    "from cuml.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import random\n",
    "random.seed(33)\n",
    "\n",
    "def recursive_loops(arrays, results, depth=0, current=[]):\n",
    "    if depth == len(arrays):\n",
    "        results.append(current)\n",
    "        return\n",
    "    for item in arrays[depth]:\n",
    "        recursive_loops(arrays, results, depth + 1, current + [item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"/mnt/c/Users/domin/Desktop/TFM/output/\"\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Date.of.Transfer</th>\n",
       "      <th>Property.Type</th>\n",
       "      <th>Old.New</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Town.City</th>\n",
       "      <th>District</th>\n",
       "      <th>County</th>\n",
       "      <th>org_property_type</th>\n",
       "      <th>org_duration</th>\n",
       "      <th>...</th>\n",
       "      <th>is_isle</th>\n",
       "      <th>spatial_cluster</th>\n",
       "      <th>distance_to_london</th>\n",
       "      <th>log_price_compare_london</th>\n",
       "      <th>log_q1</th>\n",
       "      <th>log_q3</th>\n",
       "      <th>log_mean_price</th>\n",
       "      <th>log_year_to_year_change</th>\n",
       "      <th>org_mean_price</th>\n",
       "      <th>org_price_boxcox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300000</td>\n",
       "      <td>1995-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>Nuevo</td>\n",
       "      <td>1</td>\n",
       "      <td>london</td>\n",
       "      <td>merton</td>\n",
       "      <td>greater london</td>\n",
       "      <td>Piso/Apartamento</td>\n",
       "      <td>Alquiler</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057229</td>\n",
       "      <td>1.687222</td>\n",
       "      <td>-1.338552</td>\n",
       "      <td>3.112323</td>\n",
       "      <td>-1.383724</td>\n",
       "      <td>-0.673885</td>\n",
       "      <td>-0.848781</td>\n",
       "      <td>-0.732369</td>\n",
       "      <td>112464.285714</td>\n",
       "      <td>14.442841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70500</td>\n",
       "      <td>1995-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>Segunda_mano</td>\n",
       "      <td>0</td>\n",
       "      <td>horsham</td>\n",
       "      <td>horsham</td>\n",
       "      <td>west sussex</td>\n",
       "      <td>Semi-adosado</td>\n",
       "      <td>Propiedad</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057229</td>\n",
       "      <td>-0.827957</td>\n",
       "      <td>-0.996887</td>\n",
       "      <td>1.007904</td>\n",
       "      <td>-1.383724</td>\n",
       "      <td>-0.673885</td>\n",
       "      <td>-0.848781</td>\n",
       "      <td>-0.732369</td>\n",
       "      <td>112464.285714</td>\n",
       "      <td>12.583137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35000</td>\n",
       "      <td>1995-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>Segunda_mano</td>\n",
       "      <td>0</td>\n",
       "      <td>dewsbury</td>\n",
       "      <td>kirklees</td>\n",
       "      <td>west yorkshire</td>\n",
       "      <td>Adosado</td>\n",
       "      <td>Propiedad</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057229</td>\n",
       "      <td>0.848829</td>\n",
       "      <td>0.928138</td>\n",
       "      <td>0.003143</td>\n",
       "      <td>-1.383724</td>\n",
       "      <td>-0.673885</td>\n",
       "      <td>-0.848781</td>\n",
       "      <td>-0.732369</td>\n",
       "      <td>112464.285714</td>\n",
       "      <td>11.703994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41000</td>\n",
       "      <td>1995-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>Segunda_mano</td>\n",
       "      <td>0</td>\n",
       "      <td>gloucester</td>\n",
       "      <td>gloucester</td>\n",
       "      <td>gloucestershire</td>\n",
       "      <td>Semi-adosado</td>\n",
       "      <td>Propiedad</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057229</td>\n",
       "      <td>-0.827957</td>\n",
       "      <td>-0.072118</td>\n",
       "      <td>0.228881</td>\n",
       "      <td>-1.383724</td>\n",
       "      <td>-0.673885</td>\n",
       "      <td>-0.848781</td>\n",
       "      <td>-0.732369</td>\n",
       "      <td>112464.285714</td>\n",
       "      <td>11.901504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125000</td>\n",
       "      <td>1995-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>Segunda_mano</td>\n",
       "      <td>0</td>\n",
       "      <td>macclesfield</td>\n",
       "      <td>macclesfield</td>\n",
       "      <td>cheshire</td>\n",
       "      <td>Unifamiliar</td>\n",
       "      <td>Propiedad</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057229</td>\n",
       "      <td>-0.827957</td>\n",
       "      <td>0.698371</td>\n",
       "      <td>1.837439</td>\n",
       "      <td>-1.383724</td>\n",
       "      <td>-0.673885</td>\n",
       "      <td>-0.848781</td>\n",
       "      <td>-0.732369</td>\n",
       "      <td>112464.285714</td>\n",
       "      <td>13.311820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Price Date.of.Transfer  Property.Type       Old.New  Duration  \\\n",
       "0  300000       1995-01-01              3         Nuevo         1   \n",
       "1   70500       1995-01-01              1  Segunda_mano         0   \n",
       "2   35000       1995-01-01              0  Segunda_mano         0   \n",
       "3   41000       1995-01-01              1  Segunda_mano         0   \n",
       "4  125000       1995-01-01              2  Segunda_mano         0   \n",
       "\n",
       "      Town.City      District           County org_property_type org_duration  \\\n",
       "0        london        merton   greater london  Piso/Apartamento     Alquiler   \n",
       "1       horsham       horsham      west sussex      Semi-adosado    Propiedad   \n",
       "2      dewsbury      kirklees   west yorkshire           Adosado    Propiedad   \n",
       "3    gloucester    gloucester  gloucestershire      Semi-adosado    Propiedad   \n",
       "4  macclesfield  macclesfield         cheshire       Unifamiliar    Propiedad   \n",
       "\n",
       "   ...   is_isle  spatial_cluster  distance_to_london  \\\n",
       "0  ... -0.057229         1.687222           -1.338552   \n",
       "1  ... -0.057229        -0.827957           -0.996887   \n",
       "2  ... -0.057229         0.848829            0.928138   \n",
       "3  ... -0.057229        -0.827957           -0.072118   \n",
       "4  ... -0.057229        -0.827957            0.698371   \n",
       "\n",
       "   log_price_compare_london    log_q1    log_q3  log_mean_price  \\\n",
       "0                  3.112323 -1.383724 -0.673885       -0.848781   \n",
       "1                  1.007904 -1.383724 -0.673885       -0.848781   \n",
       "2                  0.003143 -1.383724 -0.673885       -0.848781   \n",
       "3                  0.228881 -1.383724 -0.673885       -0.848781   \n",
       "4                  1.837439 -1.383724 -0.673885       -0.848781   \n",
       "\n",
       "   log_year_to_year_change  org_mean_price  org_price_boxcox  \n",
       "0                -0.732369   112464.285714         14.442841  \n",
       "1                -0.732369   112464.285714         12.583137  \n",
       "2                -0.732369   112464.285714         11.703994  \n",
       "3                -0.732369   112464.285714         11.901504  \n",
       "4                -0.732369   112464.285714         13.311820  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_pd = pd.read_pickle(input_path + \"train_df_pipeline.pkl\")\n",
    "test_df_pd = pd.read_pickle(input_path + \"test_df_pipeline.pkl\")\n",
    "\n",
    "# train_df_pd = pd.read_pickle(input_path + \"train_df.pkl\")\n",
    "# test_df_pd = pd.read_pickle(input_path + \"test_df.pkl\")\n",
    "\n",
    "train_df_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for var in test_df_pd.columns[test_df_pd.isnull().sum()>0]:\n",
    "#     test_df_pd.loc[test_df_pd[var].isnull(), var] = np.mean(test_df_pd[var])\n",
    "test_df_pd.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Date.of.Transfer</th>\n",
       "      <th>Property.Type</th>\n",
       "      <th>Old.New</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Town.City</th>\n",
       "      <th>District</th>\n",
       "      <th>County</th>\n",
       "      <th>org_property_type</th>\n",
       "      <th>org_duration</th>\n",
       "      <th>...</th>\n",
       "      <th>is_isle</th>\n",
       "      <th>spatial_cluster</th>\n",
       "      <th>distance_to_london</th>\n",
       "      <th>log_price_compare_london</th>\n",
       "      <th>log_q1</th>\n",
       "      <th>log_q3</th>\n",
       "      <th>log_mean_price</th>\n",
       "      <th>log_year_to_year_change</th>\n",
       "      <th>org_mean_price</th>\n",
       "      <th>org_price_boxcox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300000</td>\n",
       "      <td>1995-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>Nuevo</td>\n",
       "      <td>1</td>\n",
       "      <td>london</td>\n",
       "      <td>merton</td>\n",
       "      <td>greater london</td>\n",
       "      <td>Piso/Apartamento</td>\n",
       "      <td>Alquiler</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057229</td>\n",
       "      <td>1.687222</td>\n",
       "      <td>-1.338552</td>\n",
       "      <td>3.112323</td>\n",
       "      <td>-1.383724</td>\n",
       "      <td>-0.673885</td>\n",
       "      <td>-0.848781</td>\n",
       "      <td>-0.732369</td>\n",
       "      <td>112464.285714</td>\n",
       "      <td>14.442841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70500</td>\n",
       "      <td>1995-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>Segunda_mano</td>\n",
       "      <td>0</td>\n",
       "      <td>horsham</td>\n",
       "      <td>horsham</td>\n",
       "      <td>west sussex</td>\n",
       "      <td>Semi-adosado</td>\n",
       "      <td>Propiedad</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057229</td>\n",
       "      <td>-0.827957</td>\n",
       "      <td>-0.996887</td>\n",
       "      <td>1.007904</td>\n",
       "      <td>-1.383724</td>\n",
       "      <td>-0.673885</td>\n",
       "      <td>-0.848781</td>\n",
       "      <td>-0.732369</td>\n",
       "      <td>112464.285714</td>\n",
       "      <td>12.583137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35000</td>\n",
       "      <td>1995-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>Segunda_mano</td>\n",
       "      <td>0</td>\n",
       "      <td>dewsbury</td>\n",
       "      <td>kirklees</td>\n",
       "      <td>west yorkshire</td>\n",
       "      <td>Adosado</td>\n",
       "      <td>Propiedad</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057229</td>\n",
       "      <td>0.848829</td>\n",
       "      <td>0.928138</td>\n",
       "      <td>0.003143</td>\n",
       "      <td>-1.383724</td>\n",
       "      <td>-0.673885</td>\n",
       "      <td>-0.848781</td>\n",
       "      <td>-0.732369</td>\n",
       "      <td>112464.285714</td>\n",
       "      <td>11.703994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41000</td>\n",
       "      <td>1995-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>Segunda_mano</td>\n",
       "      <td>0</td>\n",
       "      <td>gloucester</td>\n",
       "      <td>gloucester</td>\n",
       "      <td>gloucestershire</td>\n",
       "      <td>Semi-adosado</td>\n",
       "      <td>Propiedad</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057229</td>\n",
       "      <td>-0.827957</td>\n",
       "      <td>-0.072118</td>\n",
       "      <td>0.228881</td>\n",
       "      <td>-1.383724</td>\n",
       "      <td>-0.673885</td>\n",
       "      <td>-0.848781</td>\n",
       "      <td>-0.732369</td>\n",
       "      <td>112464.285714</td>\n",
       "      <td>11.901504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125000</td>\n",
       "      <td>1995-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>Segunda_mano</td>\n",
       "      <td>0</td>\n",
       "      <td>macclesfield</td>\n",
       "      <td>macclesfield</td>\n",
       "      <td>cheshire</td>\n",
       "      <td>Unifamiliar</td>\n",
       "      <td>Propiedad</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057229</td>\n",
       "      <td>-0.827957</td>\n",
       "      <td>0.698371</td>\n",
       "      <td>1.837439</td>\n",
       "      <td>-1.383724</td>\n",
       "      <td>-0.673885</td>\n",
       "      <td>-0.848781</td>\n",
       "      <td>-0.732369</td>\n",
       "      <td>112464.285714</td>\n",
       "      <td>13.311820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Price Date.of.Transfer  Property.Type       Old.New  Duration  \\\n",
       "0  300000       1995-01-01              3         Nuevo         1   \n",
       "1   70500       1995-01-01              1  Segunda_mano         0   \n",
       "2   35000       1995-01-01              0  Segunda_mano         0   \n",
       "3   41000       1995-01-01              1  Segunda_mano         0   \n",
       "4  125000       1995-01-01              2  Segunda_mano         0   \n",
       "\n",
       "      Town.City      District           County org_property_type org_duration  \\\n",
       "0        london        merton   greater london  Piso/Apartamento     Alquiler   \n",
       "1       horsham       horsham      west sussex      Semi-adosado    Propiedad   \n",
       "2      dewsbury      kirklees   west yorkshire           Adosado    Propiedad   \n",
       "3    gloucester    gloucester  gloucestershire      Semi-adosado    Propiedad   \n",
       "4  macclesfield  macclesfield         cheshire       Unifamiliar    Propiedad   \n",
       "\n",
       "   ...   is_isle  spatial_cluster  distance_to_london  \\\n",
       "0  ... -0.057229         1.687222           -1.338552   \n",
       "1  ... -0.057229        -0.827957           -0.996887   \n",
       "2  ... -0.057229         0.848829            0.928138   \n",
       "3  ... -0.057229        -0.827957           -0.072118   \n",
       "4  ... -0.057229        -0.827957            0.698371   \n",
       "\n",
       "   log_price_compare_london    log_q1    log_q3  log_mean_price  \\\n",
       "0                  3.112323 -1.383724 -0.673885       -0.848781   \n",
       "1                  1.007904 -1.383724 -0.673885       -0.848781   \n",
       "2                  0.003143 -1.383724 -0.673885       -0.848781   \n",
       "3                  0.228881 -1.383724 -0.673885       -0.848781   \n",
       "4                  1.837439 -1.383724 -0.673885       -0.848781   \n",
       "\n",
       "   log_year_to_year_change  org_mean_price  org_price_boxcox  \n",
       "0                -0.732369   112464.285714         14.442841  \n",
       "1                -0.732369   112464.285714         12.583137  \n",
       "2                -0.732369   112464.285714         11.703994  \n",
       "3                -0.732369   112464.285714         11.901504  \n",
       "4                -0.732369   112464.285714         13.311820  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Date.of.Transfer</th>\n",
       "      <th>Property.Type</th>\n",
       "      <th>Old.New</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Town.City</th>\n",
       "      <th>District</th>\n",
       "      <th>County</th>\n",
       "      <th>org_property_type</th>\n",
       "      <th>org_duration</th>\n",
       "      <th>...</th>\n",
       "      <th>job_density</th>\n",
       "      <th>density_population_hectare</th>\n",
       "      <th>is_coastal</th>\n",
       "      <th>is_isle</th>\n",
       "      <th>spatial_cluster</th>\n",
       "      <th>distance_to_london</th>\n",
       "      <th>log_price_compare_london</th>\n",
       "      <th>log_year_to_year_change</th>\n",
       "      <th>org_mean_price</th>\n",
       "      <th>org_price_boxcox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>234000</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>Segunda_mano</td>\n",
       "      <td>1</td>\n",
       "      <td>liverpool</td>\n",
       "      <td>knowsley</td>\n",
       "      <td>merseyside</td>\n",
       "      <td>Unifamiliar</td>\n",
       "      <td>Alquiler</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051706</td>\n",
       "      <td>-0.122306</td>\n",
       "      <td>-0.603309</td>\n",
       "      <td>-0.057229</td>\n",
       "      <td>1.128294</td>\n",
       "      <td>1.086680</td>\n",
       "      <td>-0.398926</td>\n",
       "      <td>1.918864</td>\n",
       "      <td>273204.058599</td>\n",
       "      <td>14.119733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>255000</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>Segunda_mano</td>\n",
       "      <td>1</td>\n",
       "      <td>st ives</td>\n",
       "      <td>cornwall</td>\n",
       "      <td>cornwall</td>\n",
       "      <td>Piso/Apartamento</td>\n",
       "      <td>Alquiler</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029515</td>\n",
       "      <td>-0.741602</td>\n",
       "      <td>1.657527</td>\n",
       "      <td>-0.057229</td>\n",
       "      <td>-0.618358</td>\n",
       "      <td>2.215009</td>\n",
       "      <td>0.092717</td>\n",
       "      <td>1.918864</td>\n",
       "      <td>273204.058599</td>\n",
       "      <td>14.231305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>450000</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>Segunda_mano</td>\n",
       "      <td>0</td>\n",
       "      <td>harrow</td>\n",
       "      <td>harrow</td>\n",
       "      <td>greater london</td>\n",
       "      <td>Adosado</td>\n",
       "      <td>Propiedad</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.201499</td>\n",
       "      <td>1.104223</td>\n",
       "      <td>-0.603309</td>\n",
       "      <td>-0.057229</td>\n",
       "      <td>1.687222</td>\n",
       "      <td>-1.302514</td>\n",
       "      <td>1.046355</td>\n",
       "      <td>1.918864</td>\n",
       "      <td>273204.058599</td>\n",
       "      <td>14.973763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>360000</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>Segunda_mano</td>\n",
       "      <td>1</td>\n",
       "      <td>west wickham</td>\n",
       "      <td>bromley</td>\n",
       "      <td>greater london</td>\n",
       "      <td>Piso/Apartamento</td>\n",
       "      <td>Alquiler</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134924</td>\n",
       "      <td>0.026486</td>\n",
       "      <td>-0.603309</td>\n",
       "      <td>-0.057229</td>\n",
       "      <td>1.687222</td>\n",
       "      <td>-1.329589</td>\n",
       "      <td>1.075334</td>\n",
       "      <td>1.918864</td>\n",
       "      <td>273204.058599</td>\n",
       "      <td>14.681015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>590000</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>Segunda_mano</td>\n",
       "      <td>0</td>\n",
       "      <td>orpington</td>\n",
       "      <td>bromley</td>\n",
       "      <td>greater london</td>\n",
       "      <td>Semi-adosado</td>\n",
       "      <td>Propiedad</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134924</td>\n",
       "      <td>0.026486</td>\n",
       "      <td>-0.603309</td>\n",
       "      <td>-0.057229</td>\n",
       "      <td>-0.827957</td>\n",
       "      <td>-1.297941</td>\n",
       "      <td>1.075334</td>\n",
       "      <td>1.918864</td>\n",
       "      <td>273204.058599</td>\n",
       "      <td>15.330982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Price Date.of.Transfer  Property.Type       Old.New  Duration  \\\n",
       "0  234000       2021-01-01              2  Segunda_mano         1   \n",
       "1  255000       2021-01-01              3  Segunda_mano         1   \n",
       "2  450000       2021-01-01              0  Segunda_mano         0   \n",
       "3  360000       2021-01-01              3  Segunda_mano         1   \n",
       "4  590000       2021-01-01              1  Segunda_mano         0   \n",
       "\n",
       "      Town.City  District          County org_property_type org_duration  ...  \\\n",
       "0     liverpool  knowsley      merseyside       Unifamiliar     Alquiler  ...   \n",
       "1       st ives  cornwall        cornwall  Piso/Apartamento     Alquiler  ...   \n",
       "2        harrow    harrow  greater london           Adosado    Propiedad  ...   \n",
       "3  west wickham   bromley  greater london  Piso/Apartamento     Alquiler  ...   \n",
       "4     orpington   bromley  greater london      Semi-adosado    Propiedad  ...   \n",
       "\n",
       "   job_density  density_population_hectare  is_coastal   is_isle  \\\n",
       "0    -0.051706                   -0.122306   -0.603309 -0.057229   \n",
       "1    -0.029515                   -0.741602    1.657527 -0.057229   \n",
       "2    -0.201499                    1.104223   -0.603309 -0.057229   \n",
       "3    -0.134924                    0.026486   -0.603309 -0.057229   \n",
       "4    -0.134924                    0.026486   -0.603309 -0.057229   \n",
       "\n",
       "   spatial_cluster  distance_to_london  log_price_compare_london  \\\n",
       "0         1.128294            1.086680                 -0.398926   \n",
       "1        -0.618358            2.215009                  0.092717   \n",
       "2         1.687222           -1.302514                  1.046355   \n",
       "3         1.687222           -1.329589                  1.075334   \n",
       "4        -0.827957           -1.297941                  1.075334   \n",
       "\n",
       "   log_year_to_year_change  org_mean_price  org_price_boxcox  \n",
       "0                 1.918864   273204.058599         14.119733  \n",
       "1                 1.918864   273204.058599         14.231305  \n",
       "2                 1.918864   273204.058599         14.973763  \n",
       "3                 1.918864   273204.058599         14.681015  \n",
       "4                 1.918864   273204.058599         15.330982  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert pandas DataFrame to cudf DataFrame for GPU processing\n",
    "train_df = cudf.DataFrame.from_pandas(train_df_pd)\n",
    "test_df = cudf.DataFrame.from_pandas(test_df_pd)\n",
    "\n",
    "# Check the loaded data\n",
    "del train_df_pd, test_df_pd\n",
    "display(train_df.head())\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov  3 18:09:18 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.02              Driver Version: 566.03         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...    On  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   47C    P3              8W /   80W |    1387MiB /   6144MiB |      4%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1238      C   /python3.11                                 N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet\n",
    "# https://stackoverflow.com/questions/41567081/get-schema-of-parquet-file-in-python\n",
    "\n",
    "def read_parquet_schema_df(uri: str) -> pd.DataFrame:\n",
    "    \"\"\"Return a Pandas dataframe corresponding to the schema of a local URI of a parquet file.\n",
    "\n",
    "    The returned dataframe has the columns: column, pa_dtype\n",
    "    \"\"\"\n",
    "    # Ref: https://stackoverflow.com/a/64288036/\n",
    "    schema = pyarrow.parquet.read_schema(uri, memory_map=True)\n",
    "    schema = pd.DataFrame(({\"column\": name, \"pa_dtype\": str(pa_dtype)} for name, pa_dtype in zip(schema.names, schema.types)))\n",
    "    schema = schema.reindex(columns=[\"column\", \"pa_dtype\"], fill_value=pd.NA)  # Ensures columns in case the parquet file has an empty dataframe.\n",
    "    return schema\n",
    "\n",
    "train_schema = read_parquet_schema_df('../../data/parquet/train/train.parquet')\n",
    "\n",
    "all_numeric_vars = train_schema.loc[train_schema[\"pa_dtype\"].isin([\"timestamp[us]\", \"string\"]) == False, \"column\"][:-1].values # last is an index\n",
    "independent_variables = [v for v in all_numeric_vars if v not in [\"Price_boxcox\", \"Price\", \"org_price_boxcox\", \"org_mean_price\"]]\n",
    "target_variable = \"Price_boxcox\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección de variables mediante Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.loc[:, independent_variables].astype(\"float32\")\n",
    "y_train = train_df[\"Price_boxcox\"].astype(\"float32\")\n",
    "\n",
    "# X_test = test_df.loc[:, independent_variables].astype(\"float32\")\n",
    "# y_test= test_df[\"Price_boxcox\"].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un modelo Random Forest arbitrario para evaluar el aumento en el error cuando se cambian las variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.03595009073615074\n",
      "R² Score: 0.9641258716583252\n",
      "CPU times: user 26.4 s, sys: 13.9 s, total: 40.3 s\n",
      "Wall time: 38.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = cuRF(n_estimators=100, max_depth=10, random_state=33,\n",
    "         n_streams=1, bootstrap=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_train)\n",
    "\n",
    "mse = mean_squared_error(y_train, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "r2 = r2_score(y_train, y_pred)\n",
    "print(f'R² Score: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Feature    Importance\n",
      "12        price_compare_london  2.924565e-01\n",
      "31    log_price_compare_london  1.945822e-01\n",
      "3                         Year  1.099439e-01\n",
      "8               is_top_outlier  6.003292e-02\n",
      "7                           q3  3.498458e-02\n",
      "32                      log_q1  2.547155e-02\n",
      "6                           q1  2.357341e-02\n",
      "10                  mean_price  1.718652e-02\n",
      "33                      log_q3  1.199350e-02\n",
      "34              log_mean_price  8.947540e-03\n",
      "30          distance_to_london  7.655680e-03\n",
      "1                     Duration  1.510356e-03\n",
      "29             spatial_cluster  1.453992e-03\n",
      "26  density_population_hectare  1.334012e-03\n",
      "11         year_to_year_change  1.327071e-03\n",
      "35     log_year_to_year_change  1.297381e-03\n",
      "21                       BandD  1.161467e-03\n",
      "19                       BandB  6.281435e-04\n",
      "25                 job_density  4.785508e-04\n",
      "20                       BandC  2.768040e-04\n",
      "23                       BandF  2.330840e-04\n",
      "2                   First_hand  2.088510e-04\n",
      "22                       BandE  1.971684e-04\n",
      "16             people_studying  1.756698e-04\n",
      "4                        month  1.689754e-04\n",
      "18                       BandA  1.622625e-04\n",
      "17                total_houses  1.247525e-04\n",
      "27                  is_coastal  1.071058e-04\n",
      "14                total_people  1.067929e-04\n",
      "15              people_working  9.284168e-05\n",
      "24                       BandG  6.738678e-05\n",
      "0                Property.Type  4.611164e-05\n",
      "5                          day  2.508983e-05\n",
      "13                fixed_effect  1.038611e-05\n",
      "9            is_bottom_outlier  1.538545e-06\n",
      "28                     is_isle  1.266599e-07\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Calculate the baseline error on the original data\n",
    "baseline_error = mse\n",
    "\n",
    "# Step 2: Initialize a dictionary to store importance scores\n",
    "feature_importances = {}\n",
    "\n",
    "# Step 3: Calculate importance for each feature\n",
    "for col in X_train.columns:\n",
    "    # Make a copy of X to permute this feature\n",
    "    X_permuted = X_train.copy()\n",
    "    \n",
    "    # Step 4: Permute the column values (shuffling breaks the association with the target)\n",
    "    X_permuted[col] = X_permuted[col].sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Step 5: Predict with the permuted data and calculate the new error\n",
    "    permuted_predictions = model.predict(X_permuted)\n",
    "    permuted_error = mean_squared_error(y_train, permuted_predictions)\n",
    "    \n",
    "    # Step 6: Calculate importance as the increase in error from the baseline\n",
    "    feature_importance = permuted_error - baseline_error\n",
    "    feature_importances[col] = feature_importance\n",
    "\n",
    "# Convert to DataFrame for sorting and readability\n",
    "feature_importance_df = cudf.DataFrame(list(feature_importances.items()), columns=['Feature', 'Importance'])\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the feature importances\n",
    "print(feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df.to_csv(\"../../output/R/feature_importance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionamos las variables (nos basamos en el gráfico generado en R, concretamente, descartamos las variables que tienen importancia prácticamente cero):\n",
    "\n",
    "![feature importance](feature_importance.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_variables = [v for v in independent_variables if v not in \n",
    "                         (\n",
    "                             # 'log_year_to_year_change', #\n",
    "                             # 'Duration', #\n",
    "                             'BandG',\n",
    "                             'is_coastal',\n",
    "                             'BandF',\n",
    "                             'total_people',\n",
    "                             'total_houses',\n",
    "                             'people_working',\n",
    "                             'people_studying',\n",
    "                             # 'spatial_cluster',\n",
    "                             'day',\n",
    "                             'is_bottom_outlier',\n",
    "                             'is_isle',\n",
    "                             'fixed_effect',\n",
    "                             # 'year_to_year_change', #\n",
    "                             'month', #\n",
    "                             'BandA',\n",
    "                             'job_density',\n",
    "                             'First_hand', #\n",
    "                             'BandC',\n",
    "                             # 'density_population_hectare',\n",
    "                             'BandB',\n",
    "                             'Property.Type', #\n",
    "                             'BandE',\n",
    "                             # 'BandD'\n",
    "                         )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.loc[:, independent_variables].astype(\"float32\")\n",
    "y_train = train_df[\"Price_boxcox\"].astype(\"float32\")\n",
    "\n",
    "X_test = test_df.loc[:, independent_variables].astype(\"float32\")\n",
    "y_test= test_df[\"Price_boxcox\"].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov  3 18:10:13 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.02              Driver Version: 566.03         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...    On  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   50C    P3             12W /   80W |    2313MiB /   6144MiB |     11%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1238      C   /python3.11                                 N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor\n",
    "Es necesario mantener n_streams a 1, porque se asegura el efecto determinista de fijar la semilla aleatoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.5682413578033447\n",
      "R² Score: 0.23278355598449707\n",
      "CPU times: user 21.4 s, sys: 11.4 s, total: 32.8 s\n",
      "Wall time: 32.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf = cuRF(n_estimators=100, max_depth=10, random_state=33,\n",
    "         n_streams=1, bootstrap=True)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'R² Score: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make grid\n",
    "parameters = [\n",
    "    [100, 200, 300], # n_estimators\n",
    "    [3, 5, 7, 10], # max_depth\n",
    "]\n",
    "\n",
    "results = []\n",
    "recursive_loops(arrays=parameters, results=results)\n",
    "\n",
    "resultados_rf = pd.DataFrame(results)\n",
    "resultados_rf.columns = [\"n_estimators\", \"max_depth\"]\n",
    "resultados_rf[\"mse\"] = np.float32(0)\n",
    "resultados_rf[\"r2\"] = np.float32(0)\n",
    "\n",
    "resultados_rf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12"
     ]
    }
   ],
   "source": [
    "nrows = resultados_rf.shape[0]\n",
    "for idx in np.arange(nrows):\n",
    "    print(f\"\\r{idx+1}/{nrows}\", end=\"\")\n",
    "    kwargs = {\n",
    "        \"n_estimators\": resultados_rf.loc[idx, \"n_estimators\"],\n",
    "        \"max_depth\": resultados_rf.loc[idx, \"max_depth\"],\n",
    "        \"random_state\": 33,\n",
    "        \"n_streams\": 1,\n",
    "        \"bootstrap\": True, \n",
    "    }\n",
    "\n",
    "    model = cuRF(**kwargs)\n",
    "    sample_size = 100000 * 2\n",
    "    model.fit(X_train.sample(n=sample_size, random_state=33), \n",
    "              y_train.sample(n=sample_size, random_state=33))\n",
    "    \n",
    "    # Evaluate model\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    resultados_rf.loc[idx, \"mse\"] = mean_squared_error(y_test, y_pred)\n",
    "    resultados_rf.loc[idx, \"r2\"] = r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>mse</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.393211</td>\n",
       "      <td>0.469102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300</td>\n",
       "      <td>5</td>\n",
       "      <td>0.398051</td>\n",
       "      <td>0.462567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.400537</td>\n",
       "      <td>0.459212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300</td>\n",
       "      <td>3</td>\n",
       "      <td>0.443007</td>\n",
       "      <td>0.401870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>0.445286</td>\n",
       "      <td>0.398793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_estimators  max_depth       mse        r2\n",
       "0           100          5  0.393211  0.469102\n",
       "1           300          5  0.398051  0.462567\n",
       "2           200          5  0.400537  0.459212\n",
       "3           300          3  0.443007  0.401870\n",
       "4           100          3  0.445286  0.398793"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "resultados_rf = resultados_rf.sort_values(by=\"r2\", ascending=False).reset_index().drop(\"index\", axis=1)\n",
    "resultados_rf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Linear Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/domingo/miniforge3/envs/rapids-24.08/lib/python3.11/site-packages/cuml/internals/api_decorators.py:382: UserWarning: Starting from version 23.08, the new 'copy_X' parameter defaults to 'True', ensuring a copy of X is created after passing it to fit(), preventing any changes to the input, but with increased memory usage. This represents a change in behavior from previous versions. With `copy_X=False` a copy might still be created if necessary. Explicitly set 'copy_X' to either True or False to suppress this warning.\n",
      "  return init_func(self, *args, **filtered_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.2426419258117676\n",
      "R² Score: -0.6777646541595459\n",
      "CPU times: user 88.5 ms, sys: 106 ms, total: 195 ms\n",
      "Wall time: 260 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# https://docs.rapids.ai/api/cuml/stable/estimator_intro/#Linear-regression-and-R^2-score\n",
    "model = cuLR(fit_intercept = True,\n",
    "                      normalize = True,\n",
    "                      algorithm = 'eig')\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'R² Score: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search: Linear Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 5)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make grid\n",
    "parameters = [\n",
    "    [True, False], # fit_intercept\n",
    "    [True, False], # normalize\n",
    "    ['svd', 'eig', 'qr', 'svd-qr', 'svd-jacobi'] # algorithm\n",
    "]\n",
    "\n",
    "results = []\n",
    "recursive_loops(arrays=parameters, results=results)\n",
    "\n",
    "resultados_lm = pd.DataFrame(results)\n",
    "resultados_lm.columns = [\"fit_intercept\", \"normalize\", \"algorithm\"]\n",
    "resultados_lm[\"mse\"] = np.float32(0)\n",
    "resultados_lm[\"r2\"] = np.float32(0)\n",
    "\n",
    "resultados_lm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20"
     ]
    }
   ],
   "source": [
    "nrows = resultados_lm.shape[0]\n",
    "for idx in np.arange(nrows):\n",
    "    print(f\"\\r{idx+1}/{nrows}\", end=\"\")\n",
    "    kwargs = {\n",
    "        \"fit_intercept\": resultados_lm.loc[idx, \"fit_intercept\"],\n",
    "        \"normalize\": resultados_lm.loc[idx, \"normalize\"],\n",
    "        \"algorithm\": resultados_lm.loc[idx, \"algorithm\"],\n",
    "        \"copy_X\": True,\n",
    "    }\n",
    "\n",
    "    model = cuLR(**kwargs)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    resultados_lm.loc[idx, \"mse\"] = mean_squared_error(y_test, y_pred)\n",
    "    resultados_lm.loc[idx, \"r2\"] = r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_intercept</th>\n",
       "      <th>normalize</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>mse</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>eig</td>\n",
       "      <td>1.242642</td>\n",
       "      <td>-0.677765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>qr</td>\n",
       "      <td>1.279373</td>\n",
       "      <td>-0.727357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>svd</td>\n",
       "      <td>1.279373</td>\n",
       "      <td>-0.727358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>qr</td>\n",
       "      <td>1.468717</td>\n",
       "      <td>-0.983002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>svd</td>\n",
       "      <td>1.468718</td>\n",
       "      <td>-0.983003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_intercept  normalize algorithm       mse        r2\n",
       "0           True       True       eig  1.242642 -0.677765\n",
       "1           True       True        qr  1.279373 -0.727357\n",
       "2           True       True       svd  1.279373 -0.727358\n",
       "3          False       True        qr  1.468717 -0.983002\n",
       "4          False       True       svd  1.468718 -0.983003"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "resultados_lm = resultados_lm.sort_values(by=\"r2\", ascending=False).reset_index().drop(\"index\", axis=1)\n",
    "resultados_lm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### SVM (Support Vector Machine)\n",
    "\n",
    "No da buenos resultados y tiene un alto coste computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # https://docs.rapids.ai/api/cuml/stable/estimator_intro/#Linear-regression-and-R^2-score\n",
    "# model = SVR(kernel='rbf', gamma='scale', C=10, epsilon=0.1)\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate the model\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "# print(f'R² Score: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.4925053119659424\n",
      "R² Score: 0.33503925800323486\n",
      "CPU times: user 730 ms, sys: 28.7 ms, total: 759 ms\n",
      "Wall time: 763 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# https://docs.rapids.ai/api/cuml/stable/estimator_intro/#Linear-regression-and-R^2-score\n",
    "model = ElasticNet(alpha = 0.1, l1_ratio=0.5, solver='qn')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'R² Score: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search: Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 5)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make grid\n",
    "parameters = [\n",
    "    [0.1, 0.5, 0.7, 1], # alpha\n",
    "    [0.3, 0.5, 0.7, 1], # l1_ratio\n",
    "    [1e-3, 1e-4, 1e-2] # tol\n",
    "]\n",
    "\n",
    "results = []\n",
    "recursive_loops(arrays=parameters, results=results)\n",
    "\n",
    "resultados_en = pd.DataFrame(results)\n",
    "resultados_en.columns = [\"alpha\", \"l1_ratio\", \"tol\"]\n",
    "resultados_en[\"mse\"] = np.float32(0)\n",
    "resultados_en[\"r2\"] = np.float32(0)\n",
    "\n",
    "resultados_en.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48"
     ]
    }
   ],
   "source": [
    "nrows = resultados_en.shape[0]\n",
    "for idx in np.arange(nrows):\n",
    "    print(f\"\\r{idx+1}/{nrows}\", end=\"\")\n",
    "    kwargs = {\n",
    "        \"alpha\": resultados_en.loc[idx, \"alpha\"],\n",
    "        \"l1_ratio\": resultados_en.loc[idx, \"l1_ratio\"],\n",
    "        \"tol\": resultados_en.loc[idx, \"tol\"],\n",
    "    }\n",
    "\n",
    "    model = ElasticNet(**kwargs)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    resultados_en.loc[idx, \"mse\"] = mean_squared_error(y_test, y_pred)\n",
    "    resultados_en.loc[idx, \"r2\"] = r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>l1_ratio</th>\n",
       "      <th>tol</th>\n",
       "      <th>mse</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.708429</td>\n",
       "      <td>0.043508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.708429</td>\n",
       "      <td>0.043508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.708429</td>\n",
       "      <td>0.043508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.715108</td>\n",
       "      <td>0.034490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.715154</td>\n",
       "      <td>0.034428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alpha  l1_ratio     tol       mse        r2\n",
       "0    1.0       0.5  0.0100  0.708429  0.043508\n",
       "1    1.0       0.5  0.0001  0.708429  0.043508\n",
       "2    1.0       0.5  0.0010  0.708429  0.043508\n",
       "3    0.7       1.0  0.0100  0.715108  0.034490\n",
       "4    0.7       1.0  0.0010  0.715154  0.034428"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "resultados_en = resultados_en.sort_values(by=\"r2\", ascending=False).reset_index().drop(\"index\", axis=1)\n",
    "resultados_en.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/titericz/rapids-xgboost#XGBoost-GPU\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train2, X_val, y_train2, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=33)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train2, y_train2)\n",
    "dvalid = xgb.DMatrix(X_val, y_val)\n",
    "dtest = xgb.DMatrix(X_test, y_test)\n",
    "\n",
    "del X_train2, y_train2, X_val, y_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:0.98437\tvalid-rmse:0.98369\n",
      "[100]\ttrain-rmse:0.48307\tvalid-rmse:0.48333\n",
      "[200]\ttrain-rmse:0.36064\tvalid-rmse:0.36111\n",
      "[300]\ttrain-rmse:0.28050\tvalid-rmse:0.28107\n",
      "[400]\ttrain-rmse:0.23820\tvalid-rmse:0.23868\n",
      "[500]\ttrain-rmse:0.21379\tvalid-rmse:0.21422\n",
      "[600]\ttrain-rmse:0.19821\tvalid-rmse:0.19859\n",
      "[700]\ttrain-rmse:0.18857\tvalid-rmse:0.18894\n",
      "[800]\ttrain-rmse:0.17694\tvalid-rmse:0.17728\n",
      "[900]\ttrain-rmse:0.16990\tvalid-rmse:0.17023\n",
      "[1000]\ttrain-rmse:0.16262\tvalid-rmse:0.16292\n",
      "[1100]\ttrain-rmse:0.15643\tvalid-rmse:0.15671\n",
      "[1200]\ttrain-rmse:0.15111\tvalid-rmse:0.15138\n",
      "[1300]\ttrain-rmse:0.14704\tvalid-rmse:0.14730\n",
      "[1400]\ttrain-rmse:0.14419\tvalid-rmse:0.14444\n",
      "[1500]\ttrain-rmse:0.13996\tvalid-rmse:0.14020\n",
      "[1600]\ttrain-rmse:0.13619\tvalid-rmse:0.13642\n",
      "[1700]\ttrain-rmse:0.13428\tvalid-rmse:0.13453\n",
      "[1800]\ttrain-rmse:0.13158\tvalid-rmse:0.13183\n",
      "[1900]\ttrain-rmse:0.12875\tvalid-rmse:0.12899\n",
      "[1999]\ttrain-rmse:0.12727\tvalid-rmse:0.12751\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "params = {\n",
    "    \"subsample\": 0.5,\n",
    "    \"colsample_bytree\": 0.1,\n",
    "    \"max_depth\": 3,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"nthread\": -1,\n",
    "    \"device\": \"cuda\",\n",
    "    \"max_bin\": 255, \n",
    "    'seed' : 33,\n",
    "}\n",
    "\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round = 2000,\n",
    "    evals = [(dtrain, \"train\"), (dvalid, \"valid\")],\n",
    "    verbose_eval = 100,\n",
    "    early_stopping_rounds = 50,\n",
    "    # feval=evalerror\n",
    ")\n",
    "\n",
    "# y_train[valid_idx] = model.predict(dvalid)\n",
    "y_pred = model.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.6246811151504517\n",
      "R² Score: 0.15658092498779297\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'R² Score: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search: XGBoost\n",
    "Vamos a hacer una versión personalizada para evitar exceder el límite de memoria de la GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 6)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Make grid\n",
    "# parameters = [\n",
    "#     np.arange(0.1, 1, 0.4), # subsample\n",
    "#     np.arange(0.01, 0.2, 0.05), # colsample_bytree\n",
    "#     np.arange(1, 10, 3), # maxdepth\n",
    "#     np.array([0.01, 0.05, 0.2]) # learning_rate\n",
    "# ]\n",
    "\n",
    "parameters = [\n",
    "    [0.5, 0.25], # subsample\n",
    "    [0.1], # colsample_bytree\n",
    "    [3, 5, 8], # maxdepth\n",
    "    [0.01] # learning_rate\n",
    "]\n",
    "\n",
    "results = []\n",
    "recursive_loops(arrays=parameters, results=results)\n",
    "resultados_xgboost = pd.DataFrame(results)\n",
    "resultados_xgboost.columns = [\"subsample\", \"colsample_bytree\", \n",
    "                              \"maxdepth\", \"learning_rate\"]\n",
    "resultados_xgboost[\"mse\"] = np.float32(0)\n",
    "resultados_xgboost[\"r2\"] = np.float32(0)\n",
    "resultados_xgboost.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6"
     ]
    }
   ],
   "source": [
    "nrows = resultados_xgboost.shape[0]\n",
    "for idx in np.arange(nrows):\n",
    "    print(f\"\\r{idx+1}/{nrows}\", end=\"\")\n",
    "    params = {\n",
    "        \"subsample\": resultados_xgboost.loc[idx, \"subsample\"],\n",
    "        \"colsample_bytree\": resultados_xgboost.loc[idx, \"colsample_bytree\"],\n",
    "        \"max_depth\": resultados_xgboost.loc[idx, \"maxdepth\"],\n",
    "        \"learning_rate\": resultados_xgboost.loc[idx, \"learning_rate\"],\n",
    "        \"eval_metric\": \"rmse\",\n",
    "        \"device\": \"cuda\",\n",
    "        \"max_bin\": 255, \n",
    "        'seed' : 33\n",
    "    }\n",
    "\n",
    "    model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round = 1000,\n",
    "    evals = [(dtrain, \"train\"), (dvalid, \"valid\")],\n",
    "    verbose_eval = 0,\n",
    "    early_stopping_rounds = 50\n",
    "    )\n",
    "\n",
    "    # Evaluate model\n",
    "    y_pred = model.predict(dtest)\n",
    "\n",
    "    resultados_xgboost.loc[idx, \"mse\"] = mean_squared_error(y_test, y_pred)\n",
    "    # print(f'Mean Squared Error: {mse}')\n",
    "    \n",
    "    resultados_xgboost.loc[idx, \"r2\"] = r2_score(y_test, y_pred)\n",
    "    # print(f'R² Score: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subsample</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>maxdepth</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>mse</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.378538</td>\n",
       "      <td>0.488913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.378619</td>\n",
       "      <td>0.488804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.383088</td>\n",
       "      <td>0.482770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.383344</td>\n",
       "      <td>0.482424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.383405</td>\n",
       "      <td>0.482342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subsample  colsample_bytree  maxdepth  learning_rate       mse        r2\n",
       "0       0.25               0.1         8           0.01  0.378538  0.488913\n",
       "1       0.50               0.1         8           0.01  0.378619  0.488804\n",
       "2       0.50               0.1         3           0.01  0.383088  0.482770\n",
       "3       0.50               0.1         5           0.01  0.383344  0.482424\n",
       "4       0.25               0.1         5           0.01  0.383405  0.482342"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "resultados_xgboost = resultados_xgboost.sort_values(by=\"r2\", ascending=False).reset_index().drop(\"index\", axis=1)\n",
    "resultados_xgboost.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:0.99773\tvalid-rmse:0.99705\n",
      "[100]\ttrain-rmse:0.79668\tvalid-rmse:0.79630\n",
      "[200]\ttrain-rmse:0.67120\tvalid-rmse:0.67104\n",
      "[300]\ttrain-rmse:0.57427\tvalid-rmse:0.57437\n",
      "[400]\ttrain-rmse:0.50402\tvalid-rmse:0.50430\n",
      "[500]\ttrain-rmse:0.44799\tvalid-rmse:0.44840\n",
      "[600]\ttrain-rmse:0.40294\tvalid-rmse:0.40343\n",
      "[700]\ttrain-rmse:0.37010\tvalid-rmse:0.37065\n",
      "[800]\ttrain-rmse:0.35827\tvalid-rmse:0.35880\n",
      "[900]\ttrain-rmse:0.34437\tvalid-rmse:0.34491\n",
      "[1000]\ttrain-rmse:0.32949\tvalid-rmse:0.33005\n",
      "[1100]\ttrain-rmse:0.31693\tvalid-rmse:0.31749\n",
      "[1200]\ttrain-rmse:0.30200\tvalid-rmse:0.30258\n",
      "[1300]\ttrain-rmse:0.28426\tvalid-rmse:0.28485\n",
      "[1400]\ttrain-rmse:0.27852\tvalid-rmse:0.27910\n",
      "[1500]\ttrain-rmse:0.27043\tvalid-rmse:0.27100\n",
      "[1600]\ttrain-rmse:0.26225\tvalid-rmse:0.26282\n",
      "[1700]\ttrain-rmse:0.25565\tvalid-rmse:0.25621\n",
      "[1800]\ttrain-rmse:0.25048\tvalid-rmse:0.25104\n",
      "[1900]\ttrain-rmse:0.24399\tvalid-rmse:0.24454\n",
      "[1999]\ttrain-rmse:0.24088\tvalid-rmse:0.24142\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "results = resultados_xgboost.iloc[0,:]\n",
    "params = {\n",
    "    \"subsample\": results[\"subsample\"],\n",
    "    \"colsample_bytree\": results[\"colsample_bytree\"],\n",
    "    \"max_depth\": np.int32(results[\"maxdepth\"]),\n",
    "    \"learning_rate\": results[\"learning_rate\"],\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"device\": \"cuda\",\n",
    "    \"max_bin\": 255, \n",
    "    'seed' : 33,\n",
    "}\n",
    "\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round = 2000,\n",
    "    evals = [(dtrain, \"train\"), (dvalid, \"valid\")],\n",
    "    verbose_eval = 100,\n",
    "    early_stopping_rounds = 50,\n",
    "    # feval=evalerror\n",
    ")\n",
    "\n",
    "# y_train[valid_idx] = model.predict(dvalid)\n",
    "y_pred = model.predict(dtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.35234957933425903\n",
      "R² Score: 0.5242719054222107\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'R² Score: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dtrain, dvalid, dtest, y_pred, y_test\n",
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../../output/xgboost_final.pkl\", \"wb\") as file: # file is a variable for storing the newly created file, it can be anything.\n",
    "    pickle.dump(model, file) # Dump function is used to write the object into the created file in byte format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluar todo test\n",
    "\n",
    "No merece la pena, sale igual que con la muestra pequeña."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>pa_dtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Price</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Date.of.Transfer</td>\n",
       "      <td>timestamp[us]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Property.Type</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Old.New</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Duration</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             column       pa_dtype\n",
       "0             Price          int64\n",
       "1  Date.of.Transfer  timestamp[us]\n",
       "2     Property.Type          int64\n",
       "3           Old.New         string\n",
       "4          Duration          int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet\n",
    "# https://stackoverflow.com/questions/41567081/get-schema-of-parquet-file-in-python\n",
    "\n",
    "def read_parquet_schema_df(uri: str) -> pd.DataFrame:\n",
    "    \"\"\"Return a Pandas dataframe corresponding to the schema of a local URI of a parquet file.\n",
    "\n",
    "    The returned dataframe has the columns: column, pa_dtype\n",
    "    \"\"\"\n",
    "    # Ref: https://stackoverflow.com/a/64288036/\n",
    "    schema = pyarrow.parquet.read_schema(uri, memory_map=True)\n",
    "    schema = pd.DataFrame(({\"column\": name, \"pa_dtype\": str(pa_dtype)} for name, pa_dtype in zip(schema.names, schema.types)))\n",
    "    schema = schema.reindex(columns=[\"column\", \"pa_dtype\"], fill_value=pd.NA)  # Ensures columns in case the parquet file has an empty dataframe.\n",
    "    return schema\n",
    "\n",
    "train_schema = read_parquet_schema_df('../../data/parquet/train/train.parquet')\n",
    "train_schema.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_numeric_vars = train_schema.loc[train_schema[\"pa_dtype\"].isin([\"timestamp[us]\", \"string\"]) == False, \"column\"][:-1].values # last is an index\n",
    "# independent_variables = [v for v in all_numeric_vars if v not in [\"Price_boxcox\", \"Price\", \"org_price_boxcox\",\n",
    "#                                                                  \"org_mean_price\", \"year_to_year_change\"]]\n",
    "# target_variable = \"Price_boxcox\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(33)\n",
    "\n",
    "# # Train: random sample\n",
    "# parquet_file = pq.ParquetFile('../../data/parquet/train/train.parquet')\n",
    "# n_batches_train = np.array([1 for _ in parquet_file.iter_batches()]).sum()\n",
    "\n",
    "# indeces = list(range(n_batches_train))[1:] # First must be 0, even if it adds one more than expected\n",
    "# # n = 30\n",
    "# n = int(0.02 * len(indeces))\n",
    "# chosen_batches_train = [0] + random.sample(indeces, n)\n",
    "\n",
    "# Test and validation partitions\n",
    "parquet_file = pq.ParquetFile('../../data/parquet/test/test.parquet')\n",
    "n_batches_test = np.array([1 for _ in parquet_file.iter_batches()]).sum()\n",
    "\n",
    "indeces = list(range(n_batches_test))\n",
    "# n = int(0.25 * len(indeces))\n",
    "# n = 1\n",
    "chosen_batches_validation = 0\n",
    "chosen_batches_test = [idx for idx in indeces if idx not in [chosen_batches_validation]]\n",
    "\n",
    "def make_dDF(batch):\n",
    "    batch_df = cudf.DataFrame.from_arrow(pyarrow.Table.from_batches([batch]))\n",
    "    X = batch_df.loc[:, independent_variables].astype(\"float32\")\n",
    "    y = batch_df[\"Price_boxcox\"].astype(\"float32\")\n",
    "    return xgb.DMatrix(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 32 of 32"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "# https://stackoverflow.com/questions/59098785/is-it-possible-to-read-parquet-files-in-chunks\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\"R2\", \"MSE\"], index=np.arange(n_batches_test))\n",
    "\n",
    "parquet_file = pq.ParquetFile('../../data/parquet/test/test.parquet')\n",
    "for idx, batch in enumerate(parquet_file.iter_batches()):\n",
    "    print(f\"\\rBatch {idx} of {n_batches_test-1}\", end=\"\")\n",
    "\n",
    "    if idx not in chosen_batches_test:\n",
    "        continue\n",
    "    \n",
    "    batch_df = batch.to_pandas()\n",
    "    \n",
    "    X_test = batch_df.loc[:, independent_variables].astype(\"float32\")\n",
    "    y_test = batch_df[\"Price_boxcox\"].astype(\"float32\")\n",
    "    dtest = xgb.DMatrix(X_test, y_test)\n",
    "    del X_test\n",
    "    \n",
    "    y_pred = model.predict(dtest)\n",
    "\n",
    "    results_df.iloc[idx, 1] = mean_squared_error(y_test, y_pred)\n",
    "    results_df.iloc[idx, 0] = r2_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2     0.520859\n",
       "MSE    0.349588\n",
       "dtype: float32"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = results_df.astype(\"float32\")\n",
    "results_df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación del modelo final en zonas más generales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mensual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 32 of 32"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "# https://stackoverflow.com/questions/59098785/is-it-possible-to-read-parquet-files-in-chunks\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\"R2\", \"MSE\"], index=np.arange(n_batches_test))\n",
    "\n",
    "parquet_file = pq.ParquetFile('../../data/parquet/test/test.parquet')\n",
    "for idx, batch in enumerate(parquet_file.iter_batches()):\n",
    "    print(f\"\\rBatch {idx} of {n_batches_test-1}\", end=\"\")\n",
    "\n",
    "    # if idx not in chosen_batches_test:\n",
    "    #     continue\n",
    "    \n",
    "    batch_df = batch.to_pandas().\\\n",
    "        groupby([\"Year\", \"month\", \"District\", \"Town.City\"]).mean(numeric_only=True).\\\n",
    "            reset_index()\n",
    "    \n",
    "    X_test = batch_df.loc[:, independent_variables].astype(\"float32\")\n",
    "    y_test = batch_df[\"Price_boxcox\"].astype(\"float32\")\n",
    "    dtest = xgb.DMatrix(X_test, y_test)\n",
    "    del X_test\n",
    "    \n",
    "    y_pred = model.predict(dtest)\n",
    "\n",
    "    results_df.iloc[idx, 1] = mean_squared_error(y_test, y_pred)\n",
    "    results_df.iloc[idx, 0] = r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2     0.556684\n",
       "MSE    0.202884\n",
       "dtype: float32"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.astype(\"float32\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 32 of 32"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "# https://stackoverflow.com/questions/59098785/is-it-possible-to-read-parquet-files-in-chunks\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\"R2\", \"MSE\"], index=np.arange(n_batches_test))\n",
    "\n",
    "parquet_file = pq.ParquetFile('../../data/parquet/test/test.parquet')\n",
    "for idx, batch in enumerate(parquet_file.iter_batches()):\n",
    "    print(f\"\\rBatch {idx} of {n_batches_test-1}\", end=\"\")\n",
    "\n",
    "    # if idx not in chosen_batches_test:\n",
    "    #     continue\n",
    "    \n",
    "    batch_df = batch.to_pandas().\\\n",
    "        groupby([\"Year\", \"month\", \"District\"]).mean(numeric_only=True).\\\n",
    "            reset_index()\n",
    "    \n",
    "    X_test = batch_df.loc[:, independent_variables].astype(\"float32\")\n",
    "    y_test = batch_df[\"Price_boxcox\"].astype(\"float32\")\n",
    "    dtest = xgb.DMatrix(X_test, y_test)\n",
    "    del X_test\n",
    "    \n",
    "    y_pred = model.predict(dtest)\n",
    "\n",
    "    results_df.iloc[idx, 1] = mean_squared_error(y_test, y_pred)\n",
    "    results_df.iloc[idx, 0] = r2_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2     0.822729\n",
       "MSE    0.057350\n",
       "dtype: float32"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.astype(\"float32\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 32 of 32"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "# https://stackoverflow.com/questions/59098785/is-it-possible-to-read-parquet-files-in-chunks\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\"R2\", \"MSE\"], index=np.arange(n_batches_test))\n",
    "\n",
    "parquet_file = pq.ParquetFile('../../data/parquet/test/test.parquet')\n",
    "for idx, batch in enumerate(parquet_file.iter_batches()):\n",
    "    print(f\"\\rBatch {idx} of {n_batches_test-1}\", end=\"\")\n",
    "\n",
    "    # if idx not in chosen_batches_test:\n",
    "    #     continue\n",
    "    \n",
    "    batch_df = batch.to_pandas().\\\n",
    "        groupby([\"Year\", \"month\", \"County\"]).mean(numeric_only=True).\\\n",
    "            reset_index()\n",
    "    \n",
    "    X_test = batch_df.loc[:, independent_variables].astype(\"float32\")\n",
    "    y_test = batch_df[\"Price_boxcox\"].astype(\"float32\")\n",
    "    dtest = xgb.DMatrix(X_test, y_test)\n",
    "    del X_test\n",
    "    \n",
    "    y_pred = model.predict(dtest)\n",
    "\n",
    "    results_df.iloc[idx, 1] = mean_squared_error(y_test, y_pred)\n",
    "    results_df.iloc[idx, 0] = r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2     0.703407\n",
       "MSE    0.068884\n",
       "dtype: float32"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.astype(\"float32\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 32 of 32"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "# https://stackoverflow.com/questions/59098785/is-it-possible-to-read-parquet-files-in-chunks\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\"R2\", \"MSE\"], index=np.arange(n_batches_test))\n",
    "\n",
    "parquet_file = pq.ParquetFile('../../data/parquet/test/test.parquet')\n",
    "for idx, batch in enumerate(parquet_file.iter_batches()):\n",
    "    print(f\"\\rBatch {idx} of {n_batches_test-1}\", end=\"\")\n",
    "\n",
    "    # if idx not in chosen_batches_test:\n",
    "    #     continue\n",
    "    \n",
    "    batch_df = batch.to_pandas().\\\n",
    "        groupby([\"Year\", \"District\", \"Town.City\"]).mean(numeric_only=True).\\\n",
    "            reset_index()\n",
    "    \n",
    "    X_test = batch_df.loc[:, independent_variables].astype(\"float32\")\n",
    "    y_test = batch_df[\"Price_boxcox\"].astype(\"float32\")\n",
    "    dtest = xgb.DMatrix(X_test, y_test)\n",
    "    del X_test\n",
    "    \n",
    "    y_pred = model.predict(dtest)\n",
    "\n",
    "    results_df.iloc[idx, 1] = mean_squared_error(y_test, y_pred)\n",
    "    results_df.iloc[idx, 0] = r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2     0.563547\n",
       "MSE    0.193746\n",
       "dtype: float32"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.astype(\"float32\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 32 of 32"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "# https://stackoverflow.com/questions/59098785/is-it-possible-to-read-parquet-files-in-chunks\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\"R2\", \"MSE\"], index=np.arange(n_batches_test))\n",
    "\n",
    "parquet_file = pq.ParquetFile('../../data/parquet/test/test.parquet')\n",
    "for idx, batch in enumerate(parquet_file.iter_batches()):\n",
    "    print(f\"\\rBatch {idx} of {n_batches_test-1}\", end=\"\")\n",
    "\n",
    "    # if idx not in chosen_batches_test:\n",
    "    #     continue\n",
    "    \n",
    "    batch_df = batch.to_pandas().\\\n",
    "        groupby([\"Year\", \"District\"]).mean(numeric_only=True).\\\n",
    "            reset_index()\n",
    "    \n",
    "    X_test = batch_df.loc[:, independent_variables].astype(\"float32\")\n",
    "    y_test = batch_df[\"Price_boxcox\"].astype(\"float32\")\n",
    "    dtest = xgb.DMatrix(X_test, y_test)\n",
    "    del X_test\n",
    "    \n",
    "    y_pred = model.predict(dtest)\n",
    "\n",
    "    results_df.iloc[idx, 1] = mean_squared_error(y_test, y_pred)\n",
    "    results_df.iloc[idx, 0] = r2_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2     0.852348\n",
       "MSE    0.046635\n",
       "dtype: float32"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.astype(\"float32\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 32 of 32"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "# https://stackoverflow.com/questions/59098785/is-it-possible-to-read-parquet-files-in-chunks\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\"R2\", \"MSE\"], index=np.arange(n_batches_test))\n",
    "\n",
    "parquet_file = pq.ParquetFile('../../data/parquet/test/test.parquet')\n",
    "for idx, batch in enumerate(parquet_file.iter_batches()):\n",
    "    print(f\"\\rBatch {idx} of {n_batches_test-1}\", end=\"\")\n",
    "\n",
    "    # if idx not in chosen_batches_test:\n",
    "    #     continue\n",
    "    \n",
    "    batch_df = batch.to_pandas().\\\n",
    "        groupby([\"Year\", \"County\"]).mean(numeric_only=True).\\\n",
    "            reset_index()\n",
    "    \n",
    "    X_test = batch_df.loc[:, independent_variables].astype(\"float32\")\n",
    "    y_test = batch_df[\"Price_boxcox\"].astype(\"float32\")\n",
    "    dtest = xgb.DMatrix(X_test, y_test)\n",
    "    del X_test\n",
    "    \n",
    "    y_pred = model.predict(dtest)\n",
    "\n",
    "    results_df.iloc[idx, 1] = mean_squared_error(y_test, y_pred)\n",
    "    results_df.iloc[idx, 0] = r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2     0.736331\n",
       "MSE    0.059756\n",
       "dtype: float32"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.astype(\"float32\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 32 of 32"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "# https://stackoverflow.com/questions/59098785/is-it-possible-to-read-parquet-files-in-chunks\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\"R2\", \"MSE\"], index=np.arange(n_batches_test))\n",
    "\n",
    "parquet_file = pq.ParquetFile('../../data/parquet/test/test.parquet')\n",
    "for idx, batch in enumerate(parquet_file.iter_batches()):\n",
    "    print(f\"\\rBatch {idx} of {n_batches_test-1}\", end=\"\")\n",
    "\n",
    "    # if idx not in chosen_batches_test:\n",
    "    #     continue\n",
    "    \n",
    "    batch_df = batch.to_pandas().\\\n",
    "        groupby([\"Year\", \"month\", \"day\", \"District\", \"Town.City\"]).mean(numeric_only=True).\\\n",
    "            reset_index()\n",
    "    \n",
    "    X_test = batch_df.loc[:, independent_variables].astype(\"float32\")\n",
    "    y_test = batch_df[\"Price_boxcox\"].astype(\"float32\")\n",
    "    dtest = xgb.DMatrix(X_test, y_test)\n",
    "    del X_test\n",
    "    \n",
    "    y_pred = model.predict(dtest)\n",
    "\n",
    "    results_df.iloc[idx, 1] = mean_squared_error(y_test, y_pred)\n",
    "    results_df.iloc[idx, 0] = r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2     0.537911\n",
       "MSE    0.268691\n",
       "dtype: float32"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.astype(\"float32\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 32 of 32"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "# https://stackoverflow.com/questions/59098785/is-it-possible-to-read-parquet-files-in-chunks\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\"R2\", \"MSE\"], index=np.arange(n_batches_test))\n",
    "\n",
    "parquet_file = pq.ParquetFile('../../data/parquet/test/test.parquet')\n",
    "for idx, batch in enumerate(parquet_file.iter_batches()):\n",
    "    print(f\"\\rBatch {idx} of {n_batches_test-1}\", end=\"\")\n",
    "\n",
    "    # if idx not in chosen_batches_test:\n",
    "    #     continue\n",
    "    \n",
    "    batch_df = batch.to_pandas().\\\n",
    "        groupby([\"Year\", \"month\", \"day\", \"District\"]).mean(numeric_only=True).\\\n",
    "            reset_index()\n",
    "    \n",
    "    X_test = batch_df.loc[:, independent_variables].astype(\"float32\")\n",
    "    y_test = batch_df[\"Price_boxcox\"].astype(\"float32\")\n",
    "    dtest = xgb.DMatrix(X_test, y_test)\n",
    "    del X_test\n",
    "    \n",
    "    y_pred = model.predict(dtest)\n",
    "\n",
    "    results_df.iloc[idx, 1] = mean_squared_error(y_test, y_pred)\n",
    "    results_df.iloc[idx, 0] = r2_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2     0.671513\n",
       "MSE    0.146793\n",
       "dtype: float32"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.astype(\"float32\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 32 of 32"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "# https://stackoverflow.com/questions/59098785/is-it-possible-to-read-parquet-files-in-chunks\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\"R2\", \"MSE\"], index=np.arange(n_batches_test))\n",
    "\n",
    "parquet_file = pq.ParquetFile('../../data/parquet/test/test.parquet')\n",
    "for idx, batch in enumerate(parquet_file.iter_batches()):\n",
    "    print(f\"\\rBatch {idx} of {n_batches_test-1}\", end=\"\")\n",
    "\n",
    "    # if idx not in chosen_batches_test:\n",
    "    #     continue\n",
    "    \n",
    "    batch_df = batch.to_pandas().\\\n",
    "        groupby([\"Year\", \"month\", \"day\", \"County\"]).mean(numeric_only=True).\\\n",
    "            reset_index()\n",
    "    \n",
    "    X_test = batch_df.loc[:, independent_variables].astype(\"float32\")\n",
    "    y_test = batch_df[\"Price_boxcox\"].astype(\"float32\")\n",
    "    dtest = xgb.DMatrix(X_test, y_test)\n",
    "    del X_test\n",
    "    \n",
    "    y_pred = model.predict(dtest)\n",
    "\n",
    "    results_df.iloc[idx, 1] = mean_squared_error(y_test, y_pred)\n",
    "    results_df.iloc[idx, 0] = r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2     0.565656\n",
       "MSE    0.141738\n",
       "dtype: float32"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.astype(\"float32\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
